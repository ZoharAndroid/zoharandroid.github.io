---
layout: post
title: 大数据学习15--编译Hadoop源码
tags: [大数据]
categories: 大数据
---

前面学习Hadoop的一些内容都是利用编译好的内容来进行操作学习的，这篇博客将介绍对Hadoop源码进行编译。下面以一台克隆的服务器来进行操作，具体克隆操纵可参考[虚拟机克隆](../大数据学习/2019-12-06-大数据学习3--虚拟机克隆.md)这篇博客。


<!-- TOC -->

- [1. 前期准备工作](#1-前期准备工作)
  - [1.1. 确保CentOS能够联网](#11-确保centos能够联网)
  - [1.2. jar包的准备](#12-jar包的准备)
- [2. jar包安装](#2-jar包安装)
  - [2.1. JDK配置](#21-jdk配置)
  - [2.2. Maven解压、配置  MAVEN_HOME和PATH](#22-maven解压配置-maven_home和path)
  - [2.3. ant解压、配置  ANT _HOME和PATH](#23-ant解压配置-ant-_home和path)
    - [2.3.1. ant解压](#231-ant解压)
    - [2.3.2. ant配置](#232-ant配置)
    - [ant验证](#ant验证)
  - [安装glibc-headers和g++](#安装glibc-headers和g)
  - [安装make和cmake](#安装make和cmake)
  - [解压protobuf](#解压protobuf)
  - [安装openssl库](#安装openssl库)
  - [安装ncurses-devel库](#安装ncurses-devel库)
- [编译源码](#编译源码)
  - [解压源码到/opt/目录并切换到该目录下](#解压源码到opt目录并切换到该目录下)
  - [通过maven执行编译命令](#通过maven执行编译命令)
  - [成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下](#成功的64位hadoop包在opthadoop-272-srchadoop-disttarget下)

<!-- /TOC -->

# 1. 前期准备工作

## 1.1. 确保CentOS能够联网

按照前面的博客一路学过来看过来的同学，是应该能够保证服务器能够联网的。具体请参考这篇博客[CentOS配置](/2019-12-05-大数据学习2--CentOS配置.md)，将虚拟机以及Linux系统网络配置好即可。

可以连接外网的同学可以ping一下百度，确认一下是否能够ping通。

```
[root@hadoop101 ~]# ping www.baidu.com
PING www.a.shifen.com (14.215.177.39) 56(84) bytes of data.
64 bytes from 14.215.177.39: icmp_seq=1 ttl=128 time=42.8 ms
64 bytes from 14.215.177.39: icmp_seq=2 ttl=128 time=39.4 ms
64 bytes from 14.215.177.39: icmp_seq=3 ttl=128 time=105 ms
```

> 注意：后面都采用root用户进行操作，减少文件夹权限出现错误。

## 1.2. jar包的准备

需要准备以下内容：

1. hadoop源码：hadoop-2.7.2-src.tar.gz
2. jdk:jdk-8u144-linux-x64.tar.gz
3. apache-ant-1.9.9-bin.tar.gz（build工具，打包用的）
4. apache-maven-3.0.5-bin.tar.gz
5. protobuf-2.5.0.tar.gz（序列化的框架）

下载地址如下：

名称|地址|提取码
|:-:|:-:|:-:|
hadoop源码|链接：https://pan.baidu.com/s/1gtPsDR5Nhqky5JK2eQ91TA |d4e6
jdk|链接：https://pan.baidu.com/s/1VOKQQQBUCFD6xLOIBUQZuQ |提取码：duzt 
apache-ant|链接：https://pan.baidu.com/s/1xs0RxiQwqardO0yndjQ_kw |提取码：sqxs
apache-maven|链接：https://pan.baidu.com/s/1n4j5ei9nsnM0DAo5YAjEWA |提取码：1pwj
protobuf|链接：https://pan.baidu.com/s/1mPm5g-AFPa_SBx8Skmfb_Q |提取码：9p1j

下载完成后，上传到服务器中。

```
[root@hadoop101 software]# rz
```
> 这里使用rz命令来上传到服务器中。

# 2. jar包安装

所有操作都是在root用户下进行操作的。

## 2.1. JDK配置

把jdk安装和配置好，具体参考[安装和配置jdk和hadoop](/2019-12-06-大数据学习4--安装和配置jdk和hadoop.md)这篇博客中的jdk配置。

这里我输入`java -version`可以查看到java环境已经配置好了，如下显示所示，这里不再赘述了，配置好了的同学这一步也可以直接跳过了。

```
[root@hadoop101 module]# java -version
java version "1.8.0_144"
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
```

## 2.2. Maven解压、配置  MAVEN_HOME和PATH

Maven解压到`/opt/module`目录下：

```
[root@hadoop101 software]# tar -xvzf apache-maven-3.0.5-bin.tar.gz -C /opt/module/
```

切换到maven目录下，然后进行配置：
```
[root@hadoop101 apache-maven-3.0.5]# cd /opt/module/apache-maven-3.0.5/
[root@hadoop101 apache-maven-3.0.5]# vim conf/settings.xml
```

在配置文件下找到`mirrors`节点，然后输入`mirror`内容。
```html
  <mirrors>
    <!-- mirror
     |
    <mirror>
      <id>mirrorId</id>
      <mirrorOf>repositoryId</mirrorOf>
      <name>Human Readable Name for this Mirror.</name>
      <url>http://my.repository.com/repo/path</url>
    </mirror>
     -->
        <mirror>
                <id>nexus-aliyun</id>
                <mirrorOf>central</mirrorOf>
                <name>Nexus aliyun</name>
                <url>http://maven.aliyun.com/nexus/content/groups/public</url>
        </mirror>
  </mirrors>
  ```

将Maven加入到环境变量之中：

```
[root@hadoop101 apache-maven-3.0.5]# vim /etc/profile
# 3. MAVEN_HOME
export MAVEN_HOME=/opt/module/apache-maven-3.0.5
export PATH=$PATH:$MAVEN_HOME/bin

[root@hadoop101 apache-maven-3.0.5]# source /etc/profile

```

验证：
```
[root@hadoop101 apache-maven-3.0.5]# mvn -version
Apache Maven 3.0.5 (r01de14724cdef164cd33c7c8c2fe155faf9602da; 2013-02-19 21:51:28+0800)
Maven home: /opt/module/apache-maven-3.0.5
Java version: 1.8.0_144, vendor: Oracle Corporation
Java home: /opt/module/jdk1.8.0_144/jre
Default locale: zh_CN, platform encoding: UTF-8
OS name: "linux", version: "2.6.32-754.el6.x86_64", arch: "amd64", family: "unix"
```

## 2.3. ant解压、配置  ANT _HOME和PATH

### 2.3.1. ant解压

```
[root@hadoop101 apache-maven-3.0.5]# cd /opt/software/
[root@hadoop101 software]# tar -xvzf apache-ant-1.9.9-bin.tar.gz -C /opt/module/ 
```

### 2.3.2. ant配置

```
[root@hadoop101 apache-ant-1.9.9]# vim /etc/profile

# 4. ANT_HOME
export ANT_HOME=/opt/module/apache-ant-1.9.9
export PATH=$PATH:$ANT_HOME/bin

[root@hadoop101 apache-ant-1.9.9]# source /etc/profile
```

### ant验证

```
[root@hadoop101 apache-ant-1.9.9]# ant -version
Apache Ant(TM) version 1.9.9 compiled on February 2 2017
```

## 安装glibc-headers和g++ 

```
[root@hadoop101 ~]# yum install glibc-headers
[root@hadoop101 ~]# yum install gcc-c++
```

## 安装make和cmake

```
[root@hadoop101 ~]# yum install make
[root@hadoop101 ~]# yum install cmake
```

## 解压protobuf

解压后进入protobuf主目录

```
[root@hadoop101 software]# tar -xzvf protobuf-2.5.0.tar.gz -C /opt/module/
[root@hadoop101 software]# cd /opt/module/protobuf-2.5.0/
```

接下输入下面的命令来编译protobuf并进行安装:

```
[root@hadoop101 protobuf-2.5.0]# ./configure 
[root@hadoop101 protobuf-2.5.0]# make
[root@hadoop101 protobuf-2.5.0]# make check
[root@hadoop101 protobuf-2.5.0]# make install
[root@hadoop101 protobuf-2.5.0]# ldconfig
```

将protobuf加入环境变量：

```
[root@hadoop101 protobuf-2.5.0]# vim /etc/profile
# 5. LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0
export PATH=$PATH:$LD_LIBRARY_PATH

[root@hadoop101 protobuf-2.5.0]# source /etc/profile
```

验证：
```
[root@hadoop101 protobuf-2.5.0]# protoc --version
libprotoc 2.5.0
```

## 安装openssl库

```
[root@hadoop101 protobuf-2.5.0]# yum install openssl-devel
```

## 安装ncurses-devel库

```
[root@hadoop101 protobuf-2.5.0]# yum install ncurses-devel
```

# 编译源码

## 解压源码到/opt/目录并切换到该目录下

```
[root@hadoop101 opt]# tar -xvzf hadoop-2.7.2-src.tar.gz -C /opt/

[root@hadoop101 opt]# cd /opt/hadoop-2.7.2-src/
[root@hadoop101 hadoop-2.7.2-src]# pwd
/opt/hadoop-2.7.2-src
```

## 通过maven执行编译命令

```
[root@hadoop101 hadoop-2.7.2-src]# mvn package -Pdist,native -DskipTests -Dtar
```

接下来就是默默的等待了。

## 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下

[root@hadoop101 target]# pwd
/opt/hadoop-2.7.2-src/hadoop-dist/target


